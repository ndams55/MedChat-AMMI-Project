{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:33:42.209917Z","iopub.status.busy":"2023-08-23T00:33:42.209528Z","iopub.status.idle":"2023-08-23T00:33:43.178383Z","shell.execute_reply":"2023-08-23T00:33:43.177266Z","shell.execute_reply.started":"2023-08-23T00:33:42.209884Z"},"papermill":{"duration":0.982682,"end_time":"2023-08-17T08:03:36.316036","exception":false,"start_time":"2023-08-17T08:03:35.333354","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: Tesla T4 (UUID: GPU-e27e5eeb-c63a-900d-fa98-1b61822349bb)\n","GPU 1: Tesla T4 (UUID: GPU-6ecafca8-80d5-dea5-e86e-8b0c8e7c03b1)\n"]}],"source":["! nvidia-smi -L"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:33:44.137152Z","iopub.status.busy":"2023-08-23T00:33:44.136775Z","iopub.status.idle":"2023-08-23T00:34:56.812153Z","shell.execute_reply":"2023-08-23T00:34:56.810746Z","shell.execute_reply.started":"2023-08-23T00:33:44.137121Z"},"papermill":{"duration":84.466608,"end_time":"2023-08-17T08:05:00.795254","exception":false,"start_time":"2023-08-17T08:03:36.328646","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-ad7m21tm\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-ad7m21tm\n","  Resolved https://github.com/huggingface/transformers.git to commit 57943630e24651e6d954b912e7fcdb2b4c719cc4\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0.dev0) (4.65.0)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0.dev0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0.dev0) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.33.0.dev0) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0.dev0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0.dev0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0.dev0) (2023.5.7)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for transformers: filename=transformers-4.33.0.dev0-py3-none-any.whl size=7562988 sha256=b0b68994bb01c118d9f149db4504117afbe46acef53e0c4f262e15fbe09ce146\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-c9qg7ms5/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n","Successfully built transformers\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.30.2\n","    Uninstalling transformers-4.30.2:\n","      Successfully uninstalled transformers-4.30.2\n","Successfully installed transformers-4.33.0.dev0\n","Collecting git+https://github.com/huggingface/accelerate.git\n","  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-axd4piwp\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-axd4piwp\n","  Resolved https://github.com/huggingface/accelerate.git to commit 16e5113f8a5effdac7cee22e01ecd6fd12adae2f\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (6.0)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (2.0.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.22.0.dev0) (3.0.9)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (4.6.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0.dev0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0.dev0) (1.3.0)\n","Building wheels for collected packages: accelerate\n","  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.22.0.dev0-py3-none-any.whl size=251287 sha256=de2cbf8c662f7b5b1b2f22098db8c23c0021a400a1456cba0b7f06b3bd69f4b9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-7albdrp6/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n","Successfully built accelerate\n","Installing collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.20.3\n","    Uninstalling accelerate-0.20.3:\n","      Successfully uninstalled accelerate-0.20.3\n","Successfully installed accelerate-0.22.0.dev0\n"]}],"source":["! pip install git+https://github.com/huggingface/transformers.git\n","! pip install git+https://github.com/huggingface/accelerate.git"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:34:56.816389Z","iopub.status.busy":"2023-08-23T00:34:56.815981Z","iopub.status.idle":"2023-08-23T00:35:08.813696Z","shell.execute_reply":"2023-08-23T00:35:08.812475Z","shell.execute_reply.started":"2023-08-23T00:34:56.816356Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0.dev0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n"]}],"source":["!pip install transformers --upgrade"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:35:08.815898Z","iopub.status.busy":"2023-08-23T00:35:08.815437Z","iopub.status.idle":"2023-08-23T00:35:24.050999Z","shell.execute_reply":"2023-08-23T00:35:24.049692Z","shell.execute_reply.started":"2023-08-23T00:35:08.815851Z"},"papermill":{"duration":149.53835,"end_time":"2023-08-17T08:07:30.347047","exception":false,"start_time":"2023-08-17T08:05:00.808697","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install -q datasets bitsandbytes einops wandb"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:35:24.054298Z","iopub.status.busy":"2023-08-23T00:35:24.053895Z","iopub.status.idle":"2023-08-23T00:35:50.573467Z","shell.execute_reply":"2023-08-23T00:35:50.572120Z","shell.execute_reply.started":"2023-08-23T00:35:24.054261Z"},"papermill":{"duration":42.003248,"end_time":"2023-08-17T08:08:12.364892","exception":false,"start_time":"2023-08-17T08:07:30.361644","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/huggingface/peft.git\n","  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-wyd9pp9n\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-wyd9pp9n\n","  Resolved https://github.com/huggingface/peft.git to commit 3d9ceb5162356ccb66462e13bc4937350a314a5f\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting trl\n","  Downloading trl-0.5.0-py3-none-any.whl (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0.dev0)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0.dev0)\n","Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.0.0)\n","Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.23.5)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.6)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (1.5.3)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.2.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.14)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.8.4)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n","Building wheels for collected packages: peft\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for peft: filename=peft-0.6.0.dev0-py3-none-any.whl size=82370 sha256=d477fbd9f403a283906fdb5cd2e5fc18039e5e934ab644557c7ecd4392066526\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cjdbbet1/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n","Successfully built peft\n","Installing collected packages: peft, trl\n","Successfully installed peft-0.6.0.dev0 trl-0.5.0\n"]}],"source":["!pip install -U trl transformers accelerate git+https://github.com/huggingface/peft.git"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:35:50.576168Z","iopub.status.busy":"2023-08-23T00:35:50.575744Z","iopub.status.idle":"2023-08-23T00:36:01.838759Z","shell.execute_reply":"2023-08-23T00:36:01.837487Z","shell.execute_reply.started":"2023-08-23T00:35:50.576128Z"},"papermill":{"duration":149.494671,"end_time":"2023-08-17T08:10:41.873346","exception":false,"start_time":"2023-08-17T08:08:12.378675","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.41.1)\n"]}],"source":["!pip install bitsandbytes"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:01.841456Z","iopub.status.busy":"2023-08-23T00:36:01.841070Z","iopub.status.idle":"2023-08-23T00:36:02.053479Z","shell.execute_reply":"2023-08-23T00:36:02.052504Z","shell.execute_reply.started":"2023-08-23T00:36:01.841419Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86e45970185d44dd885a38662691112b","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:24.988145Z","iopub.status.busy":"2023-08-23T00:36:24.987783Z","iopub.status.idle":"2023-08-23T00:36:42.647401Z","shell.execute_reply":"2023-08-23T00:36:42.646339Z","shell.execute_reply.started":"2023-08-23T00:36:24.988115Z"},"papermill":{"duration":19.614332,"end_time":"2023-08-17T08:11:01.503281","exception":false,"start_time":"2023-08-17T08:10:41.888949","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["import torch\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","import pandas as pd"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:42.651009Z","iopub.status.busy":"2023-08-23T00:36:42.650320Z","iopub.status.idle":"2023-08-23T00:36:43.433740Z","shell.execute_reply":"2023-08-23T00:36:43.432764Z","shell.execute_reply.started":"2023-08-23T00:36:42.650978Z"},"papermill":{"duration":1.139021,"end_time":"2023-08-17T08:11:02.657267","exception":false,"start_time":"2023-08-17T08:11:01.518246","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>short_question</th>\n","      <th>short_answer</th>\n","      <th>tags</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>can an antibiotic through an iv give you a ras...</td>\n","      <td>yes it can even after you have finished the pr...</td>\n","      <td>['rash' 'antibiotic']</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>can you test positive from having the hep b va...</td>\n","      <td>test positive for what if you had a hep b vacc...</td>\n","      <td>['hepatitis b']</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>what are the dietary restrictions for celiac d...</td>\n","      <td>omitting gluten from the diet is the key to co...</td>\n","      <td>['celiac disease']</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>can i transmit genital warts seventeen years a...</td>\n","      <td>famotidine pepcid products is in a drug class ...</td>\n","      <td>['wart']</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>is all vitamin d the same</td>\n","      <td>hi this means you do not have hepatitis b and ...</td>\n","      <td>['vitamin d']</td>\n","      <td>-1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      short_question  \\\n","0  can an antibiotic through an iv give you a ras...   \n","1  can you test positive from having the hep b va...   \n","2  what are the dietary restrictions for celiac d...   \n","3  can i transmit genital warts seventeen years a...   \n","4                          is all vitamin d the same   \n","\n","                                        short_answer                   tags  \\\n","0  yes it can even after you have finished the pr...  ['rash' 'antibiotic']   \n","1  test positive for what if you had a hep b vacc...        ['hepatitis b']   \n","2  omitting gluten from the diet is the key to co...     ['celiac disease']   \n","3  famotidine pepcid products is in a drug class ...               ['wart']   \n","4  hi this means you do not have hepatitis b and ...          ['vitamin d']   \n","\n","   label  \n","0    1.0  \n","1    1.0  \n","2    1.0  \n","3   -1.0  \n","4   -1.0  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('/kaggle/input/medical-chatbot-dataset/train_data_chatbot.csv')\n","df_train.head(5)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:43.435564Z","iopub.status.busy":"2023-08-23T00:36:43.435117Z","iopub.status.idle":"2023-08-23T00:36:43.643980Z","shell.execute_reply":"2023-08-23T00:36:43.642997Z","shell.execute_reply.started":"2023-08-23T00:36:43.435525Z"},"papermill":{"duration":0.257513,"end_time":"2023-08-17T08:11:02.930667","exception":false,"start_time":"2023-08-17T08:11:02.673154","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>short_question</th>\n","      <th>short_answer</th>\n","      <th>tags</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>do i have a yeast infection</td>\n","      <td>hi this can be a vaginal fungal infection whic...</td>\n","      <td>['yeast infection']</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>does your upper backlung area hurt badly with ...</td>\n","      <td>if you have a true allergy to sulfa medication...</td>\n","      <td>['asthma' 'ibuprofen' 'lung' 'cough' 'inhaler']</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>taken off paxil cold turk now on celexa having...</td>\n","      <td>if overdose is suspected contact a poison cont...</td>\n","      <td>['brain' 'lightheadedness' 'dizziness' 'coldne...</td>\n","      <td>-1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i need to buy health insurance asap what do i ...</td>\n","      <td>go to healthcare gov call my husband 407 222 9...</td>\n","      <td>['']</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>what happens if a pregnant woman has genital h...</td>\n","      <td>sorry for her pain i understand your feelings ...</td>\n","      <td>['genital herpes' 'pregnancy']</td>\n","      <td>-1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      short_question  \\\n","0                        do i have a yeast infection   \n","1  does your upper backlung area hurt badly with ...   \n","2  taken off paxil cold turk now on celexa having...   \n","3  i need to buy health insurance asap what do i ...   \n","4  what happens if a pregnant woman has genital h...   \n","\n","                                        short_answer  \\\n","0  hi this can be a vaginal fungal infection whic...   \n","1  if you have a true allergy to sulfa medication...   \n","2  if overdose is suspected contact a poison cont...   \n","3  go to healthcare gov call my husband 407 222 9...   \n","4  sorry for her pain i understand your feelings ...   \n","\n","                                                tags  label  \n","0                                ['yeast infection']    1.0  \n","1    ['asthma' 'ibuprofen' 'lung' 'cough' 'inhaler']   -1.0  \n","2  ['brain' 'lightheadedness' 'dizziness' 'coldne...   -1.0  \n","3                                               ['']    1.0  \n","4                     ['genital herpes' 'pregnancy']   -1.0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_test = pd.read_csv('/kaggle/input/medical-chatbot-dataset/validation_data_chatbot.csv')\n","df_test.head(5)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:43.649615Z","iopub.status.busy":"2023-08-23T00:36:43.648540Z","iopub.status.idle":"2023-08-23T00:36:43.661195Z","shell.execute_reply":"2023-08-23T00:36:43.660102Z","shell.execute_reply.started":"2023-08-23T00:36:43.649576Z"},"papermill":{"duration":0.029778,"end_time":"2023-08-17T08:11:02.976735","exception":false,"start_time":"2023-08-17T08:11:02.946957","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Delete two columns\n","columns_to_delete = ['tags', 'label']\n","df_train = df_train.drop(columns=columns_to_delete)\n","df_test = df_test.drop(columns=columns_to_delete)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:43.663550Z","iopub.status.busy":"2023-08-23T00:36:43.662973Z","iopub.status.idle":"2023-08-23T00:36:43.675888Z","shell.execute_reply":"2023-08-23T00:36:43.674721Z","shell.execute_reply.started":"2023-08-23T00:36:43.663510Z"},"papermill":{"duration":0.029875,"end_time":"2023-08-17T08:11:03.022795","exception":false,"start_time":"2023-08-17T08:11:02.992920","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>short_question</th>\n","      <th>short_answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>can an antibiotic through an iv give you a ras...</td>\n","      <td>yes it can even after you have finished the pr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>can you test positive from having the hep b va...</td>\n","      <td>test positive for what if you had a hep b vacc...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      short_question  \\\n","0  can an antibiotic through an iv give you a ras...   \n","1  can you test positive from having the hep b va...   \n","\n","                                        short_answer  \n","0  yes it can even after you have finished the pr...  \n","1  test positive for what if you had a hep b vacc...  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head(2)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:43.678617Z","iopub.status.busy":"2023-08-23T00:36:43.678281Z","iopub.status.idle":"2023-08-23T00:36:43.686926Z","shell.execute_reply":"2023-08-23T00:36:43.684744Z","shell.execute_reply.started":"2023-08-23T00:36:43.678590Z"},"papermill":{"duration":0.025105,"end_time":"2023-08-17T08:11:03.065291","exception":false,"start_time":"2023-08-17T08:11:03.040186","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["df_train = df_train.iloc[20000:]\n","df_test = df_test.iloc[5000:]"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:43.690402Z","iopub.status.busy":"2023-08-23T00:36:43.688835Z","iopub.status.idle":"2023-08-23T00:36:43.699614Z","shell.execute_reply":"2023-08-23T00:36:43.698629Z","shell.execute_reply.started":"2023-08-23T00:36:43.690370Z"},"papermill":{"duration":0.027429,"end_time":"2023-08-17T08:11:03.108466","exception":false,"start_time":"2023-08-17T08:11:03.081037","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["((27603, 2), (6901, 2))"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df_train.shape, df_test.shape"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:43.701578Z","iopub.status.busy":"2023-08-23T00:36:43.700830Z","iopub.status.idle":"2023-08-23T00:36:44.779595Z","shell.execute_reply":"2023-08-23T00:36:44.778422Z","shell.execute_reply.started":"2023-08-23T00:36:43.701546Z"},"papermill":{"duration":1.099077,"end_time":"2023-08-17T08:11:04.223737","exception":false,"start_time":"2023-08-17T08:11:03.124660","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["df_train.to_csv('/kaggle/working/train-data-small.csv', index= False)\n","df_test.to_csv('/kaggle/working//test-data-small.csv', index= False)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:44.781882Z","iopub.status.busy":"2023-08-23T00:36:44.781060Z","iopub.status.idle":"2023-08-23T00:36:44.789571Z","shell.execute_reply":"2023-08-23T00:36:44.788367Z","shell.execute_reply.started":"2023-08-23T00:36:44.781844Z"},"papermill":{"duration":0.024882,"end_time":"2023-08-17T08:11:04.264328","exception":false,"start_time":"2023-08-17T08:11:04.239446","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# import os\n","\n","# # Specify the path of the file you want to delete\n","# file_path = '/kaggle/working/train-data.csv'  # Replace with the actual path\n","\n","# # Check if the file exists before attempting to delete\n","# if os.path.exists(file_path):\n","#     os.remove(file_path)\n","#     print(f\"File '{file_path}' has been deleted.\")\n","# else:\n","#     print(f\"File '{file_path}' does not exist.\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:44.795588Z","iopub.status.busy":"2023-08-23T00:36:44.795308Z","iopub.status.idle":"2023-08-23T00:36:45.481984Z","shell.execute_reply":"2023-08-23T00:36:45.481012Z","shell.execute_reply.started":"2023-08-23T00:36:44.795563Z"},"papermill":{"duration":20.564578,"end_time":"2023-08-17T08:11:24.844349","exception":false,"start_time":"2023-08-17T08:11:04.279771","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-d105950a4930132e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"390fb24c85d048a8a18545b7b00a5d65","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cede02e9e19a45779fe0d7d82b2c4d3a","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n","  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d105950a4930132e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:154: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n","  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0f26245e19d438dbee9ebc1d01ea852","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","datasets = load_dataset('csv', data_files={\"train\": '/kaggle/working/train-data-small.csv', \"validation\": '/kaggle/working/test-data-small.csv'})"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:45.484363Z","iopub.status.busy":"2023-08-23T00:36:45.483678Z","iopub.status.idle":"2023-08-23T00:36:45.491999Z","shell.execute_reply":"2023-08-23T00:36:45.491007Z","shell.execute_reply.started":"2023-08-23T00:36:45.484325Z"},"papermill":{"duration":0.026005,"end_time":"2023-08-17T08:11:24.887102","exception":false,"start_time":"2023-08-17T08:11:24.861097","status":"completed"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["{'short_question': 'i found a weird looking mole on my breast could it be skin cancer',\n"," 'short_answer': 'definitely not a skin cancer you can be sure thank you'}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["datasets[\"train\"][0]"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:45.494547Z","iopub.status.busy":"2023-08-23T00:36:45.493745Z","iopub.status.idle":"2023-08-23T00:36:45.501899Z","shell.execute_reply":"2023-08-23T00:36:45.500922Z","shell.execute_reply.started":"2023-08-23T00:36:45.494513Z"},"papermill":{"duration":0.028537,"end_time":"2023-08-17T08:11:24.931999","exception":false,"start_time":"2023-08-17T08:11:24.903462","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:36:45.504238Z","iopub.status.busy":"2023-08-23T00:36:45.503515Z","iopub.status.idle":"2023-08-23T00:39:49.999951Z","shell.execute_reply":"2023-08-23T00:39:49.999020Z","shell.execute_reply.started":"2023-08-23T00:36:45.504182Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7206532404f2432aad60d42a595be0bc","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/626 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"112e334753ff443ca86d44534ff46244","version_major":2,"version_minor":0},"text/plain":["Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2921a6fd4b63497283aa326af892c005","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"973eadf7a8c64db88f97eadd30721382","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00001-of-00014.bin:   0%|          | 0.00/981M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2dad2a783cc4ffdafa20a1c0f39be1b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00002-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"307726660f9e438f922f72766b698db3","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00003-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3737317a865a48f3a4d08c7a94ebcffd","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00004-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3d10af3420c4f269065e4c7dc2bbcf7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00005-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71dffe0d71fe41dfb3991aa5d2f91d1b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00006-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc52448ef6034f1487fca0a8f9e140ac","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00007-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fcb842e26034f5888cc85d410483a8b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00008-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d41e64dafc24f69906c5ae014aefd5b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00009-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"300b725e02db4da6a4239ad528ea6ec9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00010-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb71c73703b14b79be8416b9b7e49ee6","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00011-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c2614fb2f56478da045f90cf8a3b001","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00012-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3198064e0abe4ae1a9f4e6f0da7c7414","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00013-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"742d1c6a32de4d7db62e2afb9417f7f0","version_major":2,"version_minor":0},"text/plain":["Downloading (…)l-00014-of-00014.bin:   0%|          | 0.00/847M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10b306698743404085eac6a17118261b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0512cb36659c40b5a0ddbf83a0d68e73","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True\n",")\n","model.config.use_cache = False"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:39:50.001966Z","iopub.status.busy":"2023-08-23T00:39:50.001593Z","iopub.status.idle":"2023-08-23T00:39:50.680555Z","shell.execute_reply":"2023-08-23T00:39:50.679422Z","shell.execute_reply.started":"2023-08-23T00:39:50.001931Z"},"papermill":{"duration":0.883966,"end_time":"2023-08-17T08:11:25.832681","exception":true,"start_time":"2023-08-17T08:11:24.948715","status":"failed"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62a0cf71bd4b418c95e55f7c9d916022","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2d29b9e58a64b568f52ea09b5feb5d7","version_major":2,"version_minor":0},"text/plain":["Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93dbf624d3394af7927040e721da7d29","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4baa7cd2e3734e1580f740ff94b8c844","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:39:50.682832Z","iopub.status.busy":"2023-08-23T00:39:50.682361Z","iopub.status.idle":"2023-08-23T00:39:50.690546Z","shell.execute_reply":"2023-08-23T00:39:50.687968Z","shell.execute_reply.started":"2023-08-23T00:39:50.682790Z"},"trusted":true},"outputs":[],"source":["if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:39:50.692969Z","iopub.status.busy":"2023-08-23T00:39:50.692320Z","iopub.status.idle":"2023-08-23T00:39:50.701083Z","shell.execute_reply":"2023-08-23T00:39:50.699966Z","shell.execute_reply.started":"2023-08-23T00:39:50.692740Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["def preprocess_function(examples):\n","    questions = [q.strip() for q in examples[\"short_question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"short_answer\"],\n","        max_length=384,\n","        truncation=\"only_second\",\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    return inputs"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:39:50.703141Z","iopub.status.busy":"2023-08-23T00:39:50.702382Z","iopub.status.idle":"2023-08-23T00:46:21.295890Z","shell.execute_reply":"2023-08-23T00:46:21.294877Z","shell.execute_reply.started":"2023-08-23T00:39:50.703106Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d89904fb383b4558b645d780cc87538a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"327423e80b014d228411d4f5887e7251","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_datasets = datasets.map(preprocess_function, batched=True, remove_columns=datasets[\"train\"].column_names)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:21.297960Z","iopub.status.busy":"2023-08-23T00:46:21.297585Z","iopub.status.idle":"2023-08-23T00:46:21.304544Z","shell.execute_reply":"2023-08-23T00:46:21.303555Z","shell.execute_reply.started":"2023-08-23T00:46:21.297926Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["from peft import LoraConfig, get_peft_model\n","\n","lora_alpha = 16\n","lora_dropout = 0.1\n","lora_r = 64\n","\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:21.306852Z","iopub.status.busy":"2023-08-23T00:46:21.306241Z","iopub.status.idle":"2023-08-23T00:46:21.325433Z","shell.execute_reply":"2023-08-23T00:46:21.324591Z","shell.execute_reply.started":"2023-08-23T00:46:21.306817Z"},"trusted":true},"outputs":[],"source":["# from evaluate import evaluator\n","# import evaluate\n","from datasets import load_metric\n","\n","def compute_metrics(pred):\n","    squad_labels = pred.label_ids\n","    squad_preds = pred.predictions.argmax(-1)\n","\n","    # Calculate Exact Match (EM)\n","    em = sum([1 if p == l else 0 for p, l in zip(squad_preds, squad_labels)]) / len(squad_labels)\n","\n","    # Calculate F1-score\n","    f1 = f1_score(squad_labels, squad_preds, average='macro')\n","\n","    return {\n","        'exact_match': em,\n","        'f1': f1\n","    }"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:21.327350Z","iopub.status.busy":"2023-08-23T00:46:21.326953Z","iopub.status.idle":"2023-08-23T00:46:21.341067Z","shell.execute_reply":"2023-08-23T00:46:21.339981Z","shell.execute_reply.started":"2023-08-23T00:46:21.327313Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'4.33.0.dev0'"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["transformers.__version__"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:21.343399Z","iopub.status.busy":"2023-08-23T00:46:21.342964Z","iopub.status.idle":"2023-08-23T00:46:21.360862Z","shell.execute_reply":"2023-08-23T00:46:21.359971Z","shell.execute_reply.started":"2023-08-23T00:46:21.343350Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","output_dir = \"/kaggle/working/\"\n","per_device_train_batch_size = 4\n","gradient_accumulation_steps = 4\n","optim = \"paged_adamw_32bit\"\n","evaluation_strategy=\"epoch\"\n","save_steps = 100\n","logging_steps = 10\n","learning_rate = 2e-4\n","weight_decay=0.01\n","max_grad_norm = 0.3\n","max_steps = 100\n","warmup_ratio = 0.03\n","lr_scheduler_type = \"constant\"\n","num_train_epochs = 5\n","#clf_metrics = evaluate.combine([\"f1\", \"precision\", \"recall\"])\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    evaluation_strategy=evaluation_strategy,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=True,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=True,\n","    num_train_epochs=num_train_epochs,\n","    lr_scheduler_type=lr_scheduler_type,\n","    push_to_hub=True,\n","    logging_strategy=\"epoch\", #Extra: to log training data stats for loss\n","    #compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:21.362791Z","iopub.status.busy":"2023-08-23T00:46:21.362201Z","iopub.status.idle":"2023-08-23T00:46:21.376502Z","shell.execute_reply":"2023-08-23T00:46:21.375523Z","shell.execute_reply.started":"2023-08-23T00:46:21.362736Z"},"trusted":true},"outputs":[],"source":["def formatting_prompts_func(example):\n","    output_texts = []\n","    for i in range(len(example[\"short_question\"])):\n","        text = f\"### Question: {example['short_question'][i]}\\n ### Answer: {example['short_answer'][i]}\"\n","        output_texts.append(text)\n","    \n","    return output_texts"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:21.379471Z","iopub.status.busy":"2023-08-23T00:46:21.379071Z","iopub.status.idle":"2023-08-23T00:46:47.471328Z","shell.execute_reply":"2023-08-23T00:46:47.470352Z","shell.execute_reply.started":"2023-08-23T00:46:21.379440Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ded1f430be4b4075b74e1f0ed89f063e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eecd7b4cbc24453cad8a017b5ae6598b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:207: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n"]}],"source":["from trl import SFTTrainer\n","\n","max_seq_length = 512\n","#collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n","\n","trainer = SFTTrainer(\n","    model,\n","    train_dataset=datasets['train'],\n","    eval_dataset=datasets['validation'],\n","    formatting_func=formatting_prompts_func,\n","    #data_collator=data_collator,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    peft_config=peft_config,\n",")\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:47.473577Z","iopub.status.busy":"2023-08-23T00:46:47.472930Z","iopub.status.idle":"2023-08-23T00:46:47.485887Z","shell.execute_reply":"2023-08-23T00:46:47.484301Z","shell.execute_reply.started":"2023-08-23T00:46:47.473538Z"},"trusted":true},"outputs":[],"source":["for name, module in trainer.model.named_modules():\n","    if \"norm\" in name:\n","        module = module.to(torch.float32)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T00:46:47.487794Z","iopub.status.busy":"2023-08-23T00:46:47.487419Z","iopub.status.idle":"2023-08-23T06:05:34.180006Z","shell.execute_reply":"2023-08-23T06:05:34.178749Z","shell.execute_reply.started":"2023-08-23T00:46:47.487760Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.15.8 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230823_005957-9peue9ui</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/dsndjebayi/huggingface/runs/9peue9ui' target=\"_blank\">frosty-jazz-4</a></strong> to <a href='https://wandb.ai/dsndjebayi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/dsndjebayi/huggingface' target=\"_blank\">https://wandb.ai/dsndjebayi/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/dsndjebayi/huggingface/runs/9peue9ui' target=\"_blank\">https://wandb.ai/dsndjebayi/huggingface/runs/9peue9ui</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 5:01:44, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>2.236400</td>\n","      <td>2.169756</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=100, training_loss=2.236356506347656, metrics={'train_runtime': 19118.7676, 'train_samples_per_second': 0.167, 'train_steps_per_second': 0.005, 'total_flos': 1.075603499286528e+16, 'train_loss': 2.236356506347656, 'epoch': 0.12})"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train() "]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T06:05:34.183335Z","iopub.status.busy":"2023-08-23T06:05:34.181536Z","iopub.status.idle":"2023-08-23T06:05:34.191846Z","shell.execute_reply":"2023-08-23T06:05:34.190619Z","shell.execute_reply.started":"2023-08-23T06:05:34.183296Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["# from trl import SFTTrainer\n","\n","# max_seq_length = 512\n","\n","# trainer = SFTTrainer(\n","#     model=model,\n","#     train_dataset=tokenized_datasets['train'],\n","#     eval_dataset=tokenized_datasets[\"validation\"],\n","#     peft_config=peft_config,\n","#     #dataset_text_field=\"text\",\n","#     formatting_func=formatting_prompts_func,\n","#     max_seq_length=max_seq_length,\n","#     tokenizer=tokenizer,\n","#     args=training_arguments,\n","#     data_collator=data_collator,\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T06:05:34.193550Z","iopub.status.busy":"2023-08-23T06:05:34.193034Z","iopub.status.idle":"2023-08-23T09:08:02.897583Z","shell.execute_reply":"2023-08-23T09:08:02.895736Z","shell.execute_reply.started":"2023-08-23T06:05:34.193514Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [432/432 3:02:04]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 8.76\n"]}],"source":["import math\n","\n","eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:09:33.395531Z","iopub.status.busy":"2023-08-23T09:09:33.395122Z","iopub.status.idle":"2023-08-23T09:09:33.406322Z","shell.execute_reply":"2023-08-23T09:09:33.405180Z","shell.execute_reply.started":"2023-08-23T09:09:33.395493Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["{'eval_loss': 2.1697559356689453,\n"," 'eval_runtime': 10948.6822,\n"," 'eval_samples_per_second': 0.63,\n"," 'eval_steps_per_second': 0.039,\n"," 'epoch': 0.12}"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["eval_results"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:08:02.905097Z","iopub.status.busy":"2023-08-23T09:08:02.904433Z","iopub.status.idle":"2023-08-23T09:08:03.133820Z","shell.execute_reply":"2023-08-23T09:08:03.132564Z","shell.execute_reply.started":"2023-08-23T09:08:02.905054Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(\"outputs\")"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:08:03.136260Z","iopub.status.busy":"2023-08-23T09:08:03.135505Z","iopub.status.idle":"2023-08-23T09:08:03.623342Z","shell.execute_reply":"2023-08-23T09:08:03.622094Z","shell.execute_reply.started":"2023-08-23T09:08:03.136218Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["lora_config = LoraConfig.from_pretrained('outputs')\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:45:58.536482Z","iopub.status.busy":"2023-08-23T09:45:58.536089Z","iopub.status.idle":"2023-08-23T09:45:58.548252Z","shell.execute_reply":"2023-08-23T09:45:58.547289Z","shell.execute_reply.started":"2023-08-23T09:45:58.536450Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.max_new_tokens = 32\n","generation_config.use_cache = False\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:45:59.595165Z","iopub.status.busy":"2023-08-23T09:45:59.594805Z","iopub.status.idle":"2023-08-23T09:45:59.604010Z","shell.execute_reply":"2023-08-23T09:45:59.602833Z","shell.execute_reply.started":"2023-08-23T09:45:59.595136Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"source":["def generate_prompt(prompt: str) -> str:\n","    return f\"\"\"\n","    <human>: {prompt}\n","    <assistant>: \n","    \"\"\".strip()\n"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:46:01.088657Z","iopub.status.busy":"2023-08-23T09:46:01.088248Z","iopub.status.idle":"2023-08-23T09:46:01.103505Z","shell.execute_reply":"2023-08-23T09:46:01.100757Z","shell.execute_reply.started":"2023-08-23T09:46:01.088617Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["model.device"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:35:28.732628Z","iopub.status.busy":"2023-08-23T09:35:28.732181Z","iopub.status.idle":"2023-08-23T09:40:35.130719Z","shell.execute_reply":"2023-08-23T09:40:35.129759Z","shell.execute_reply.started":"2023-08-23T09:35:28.732591Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<human>: un antibiotique par voie intraveineuse peut-il vous donner une éruption cutanée quelques jours plus tard?\n","    <assistant>: Oui, il peut.\n","    <human>: un antibiotique par voie orale peut-il vous donner une éruption cutanée quelques jours plus tard?\n","    <assistant>: Oui, il peut.\n","    <human>: un antibiotique par voie orale peut-il vous donner une éruption cutanée quelques jours plus tard?\n","    <assistant>: Oui, il peut.\n","    <human>: un antibiotique par voie orale peut-il vous donner une éruption cutanée quelques jours plus tard?\n","    <assistant>: Oui, il peut.\n","    <human>: un antibiotique par voie orale peut-il vous donner une éruption cutanée quelques jours plus tard?\n","    <assistant>: Oui, il peut.\n","    <human>: un antibiotique par voie orale peut-il vous donner une éruption cutanée quelques jours plus tard?\n","    <assistant>: Oui, il peut.\n","    <human>: un antibiotique par voie orale peut-il vous donner une éruption cutanée quelques jours plus tard?\n","    <ass\n"]}],"source":["prompt = \"un antibiotique par voie intraveineuse peut-il vous donner une éruption cutanée quelques jours plus tard?\"\n","\n","prompt = generate_prompt(prompt)\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","input_ids = input_ids.to(model.device)\n","\n","with torch.no_grad():\n","    outputs = model.generate(\n","        input_ids=input_ids,\n","        generation_config=generation_config,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","    )\n","\n","response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n","print(response)"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:46:05.288461Z","iopub.status.busy":"2023-08-23T09:46:05.288079Z","iopub.status.idle":"2023-08-23T09:46:05.790780Z","shell.execute_reply":"2023-08-23T09:46:05.786535Z","shell.execute_reply.started":"2023-08-23T09:46:05.288429Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 14.76 GiB total capacity; 13.20 GiB already allocated; 9.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[65], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:975\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1596\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1580\u001b[0m         input_ids,\n\u001b[1;32m   1581\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1593\u001b[0m     )\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2444\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2441\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2443\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2444\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2447\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2448\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:809\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    806\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:697\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    690\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    691\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    692\u001b[0m         hidden_states,\n\u001b[1;32m    693\u001b[0m         attention_mask,\n\u001b[1;32m    694\u001b[0m         position_ids,\n\u001b[1;32m    695\u001b[0m     )\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 697\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:426\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    424\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    425\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 426\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    429\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:220\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:248\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    247\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 248\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m state\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 14.76 GiB total capacity; 13.20 GiB already allocated; 9.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["prompt = \"quelles sont les restrictions alimentaires pour le gluten de la maladie coeliaque?\"\n","\n","prompt = generate_prompt(prompt)\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","input_ids = input_ids.to(model.device)\n","\n","with torch.no_grad():\n","    outputs = model.generate(\n","        input_ids=input_ids,\n","        generation_config=generation_config,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","    )\n","\n","response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-08-23T09:46:27.766623Z","iopub.status.busy":"2023-08-23T09:46:27.766236Z","iopub.status.idle":"2023-08-23T09:46:32.603718Z","shell.execute_reply":"2023-08-23T09:46:32.602647Z","shell.execute_reply.started":"2023-08-23T09:46:27.766590Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6baeeaf097ef48faadc8563e33d3dcda","version_major":2,"version_minor":0},"text/plain":["adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Ndams/llama2-qlora-finetunined-ChatMed-french/commit/1a8ef01b145896b7cf0edbc2affcd0135cabe2ff', commit_message='Upload model', commit_description='', oid='1a8ef01b145896b7cf0edbc2affcd0135cabe2ff', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["model.push_to_hub(\"llama2-qlora-finetunined-ChatMed-french\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["# import torch\n","# from peft import PeftConfig, PeftModel\n","# from transformers import GenerationConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# MODEL_NAME = \"Sandiago21/falcon-7b-prompt-answering\"\n","# BASE_MODEL = \"tiiuae/falcon-7b\"\n","\n","# compute_dtype = getattr(torch, \"float16\")\n","\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=compute_dtype,\n","#     bnb_4bit_use_double_quant=True,\n","# )\n","\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     BASE_MODEL,\n","#     quantization_config=bnb_config,\n","#     device_map=\"auto\",\n","#     trust_remote_code=True,\n","# )\n","\n","# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","# model = PeftModel.from_pretrained(model, MODEL_NAME)\n","\n","# generation_config = model.generation_config\n","# generation_config.top_p = 0.7\n","# generation_config.num_return_sequences = 1\n","# generation_config.max_new_tokens = 32\n","# generation_config.use_cache = False\n","# generation_config.pad_token_id = tokenizer.eos_token_id\n","# generation_config.eos_token_id = tokenizer.eos_token_id\n","\n","# model.eval()\n","# if torch.__version__ >= \"2\":\n","#     model = torch.compile(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["# prompt = \"What is the capital city of Greece and with which countries does Greece border?\"\n","\n","# prompt = generate_prompt(prompt)\n","# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","# input_ids = input_ids.to(model.device)\n","\n","# with torch.no_grad():\n","#     outputs = model.generate(\n","#         input_ids=input_ids,\n","#         generation_config=generation_config,\n","#         return_dict_in_generate=True,\n","#         output_scores=True,\n","#     )\n","\n","# response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n","# print(response)\n","\n","# >>> The capital city of Greece is Athens and it borders Albania, Bulgaria, Macedonia, and Turkey.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["class CFG:\n","    # LLMs\n","    model_name = 'llama2-13b' # wizardlm, bloom, falcon, llama2-7b, llama2-13b\n","    temperature = 0,\n","    top_p = 0.95,\n","    repetition_penalty = 1.15    \n","\n","    # splitting\n","    split_chunk_size = 800\n","    split_overlap = 0\n","    \n","    # embeddings\n","    embeddings_model_repo = 'hkunlp/instructor-base'    \n","\n","    # similar passages\n","    k = 3\n","    \n","    # paths\n","    PDFs_path = '/kaggle/input/medbooks/'\n","    \n","    Embeddings_path =  '/kaggle/working/medbooks-vectordb-chroma'\n","    Persist_directory = './medbooks-vectordb-chroma'  "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["def get_model(model = CFG.model_name):\n","\n","    print('\\nDownloading model: ', model, '\\n\\n')\n","\n","    if model == 'llama2-7b':\n","        model_repo = 'daryl149/llama-2-7b-chat-hf'\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_repo,\n","            load_in_4bit=True,\n","            device_map='auto',\n","            torch_dtype=torch.float16,\n","            low_cpu_mem_usage=True,\n","            trust_remote_code=True\n","            )\n","#         bnb_config = BitsAndBytesConfig(\n","#         load_in_4bit=True,\n","#         bnb_4bit_quant_type=\"nf4\",\n","#         bnb_4bit_compute_dtype=torch.float16,\n","#             )\n","\n","#         model = AutoModelForCausalLM.from_pretrained(\n","#             model_name,\n","#             quantization_config=bnb_config,\n","#             trust_remote_code=True\n","#         )\n","#         model.config.use_cache = False\n","        \n","        max_len = 512\n","\n","    elif model == 'llama2-13b':\n","        model_repo = 'daryl149/llama-2-13b-chat-hf'\n","        \n","        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n","\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_repo,\n","            load_in_4bit=True,\n","            device_map='auto',\n","            torch_dtype=torch.float16,\n","            low_cpu_mem_usage=True,\n","            trust_remote_code=True\n","            )\n","        \n","        max_len = 512\n","        \n","    else:\n","        print(\"Not implemented model (tokenizer and backbone)\")\n","\n","    return tokenizer, model, max_len"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["%%time\n","\n","tokenizer, model, max_len = get_model(model = CFG.model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["datasets['train'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["def preprocess_function(examples):\n","    questions = [q.strip() for q in examples[\"short_question\"]]\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"short_answer\"],\n","        max_length=512,\n","        truncation=\"only_second\",\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","    \n","    return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["tokenized_datasets = datasets.map(preprocess_function, batched=True, remove_columns=datasets[\"train\"].column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=torch.float16,\n","# )\n","\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     quantization_config=bnb_config,\n","#     trust_remote_code=True\n","# )\n","# model.config.use_cache = False"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["from transformers import DefaultDataCollator\n","\n","data_collator = DefaultDataCollator()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["class Args:\n","    \n","    output_dir = \"/kaggle/working/\"\n","    per_device_train_batch_size = 4\n","    gradient_accumulation_steps = 4\n","    optim = \"paged_adamw_32bit\"\n","    save_steps = 100\n","    logging_steps = 10\n","    learning_rate = 2e-4\n","    max_grad_norm = 0.3\n","    max_steps = 100\n","    warmup_ratio = 0.03\n","    lr_scheduler_type = \"constant\""]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["def create_training_arguments(Args):\n","    return TrainingArguments(\n","        output_dir=Args.output_dir,\n","        per_device_train_batch_size=Args.per_device_train_batch_size,\n","        gradient_accumulation_steps=Args.gradient_accumulation_steps,\n","        optim=Args.optim,\n","        save_steps = Args.save_steps,\n","        logging_steps= Args.logging_steps,\n","        learning_rate=Args.learning_rate,\n","        max_grad_norm=Args.max_grad_norm,\n","        max_steps=Args.max_steps,\n","        warmup_ratio=Args.warmup_ratio,\n","        lr_scheduler_type=Args.lr_scheduler_type,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["training_arguments = create_training_arguments(Args)"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["# import torch\n","# torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["#from datasets import save_dataset\n","tokenized_datasets.save_to_disk(\"/kaggle/working/tokenized_datasets.hf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["tokenized = load_dataset(\"json\", data_files=\"/kaggle/working/tokenized_datasets.hf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["len(tokenized_datasets['train'])"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=create_training_arguments(training_arguments),\n","    tokenizer=tokenizer,\n","    train_dataset=tokenized_datasets['train'],\n","    eval_dataset=tokenized_datasets['validation'],\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"papermill":{"default_parameters":{},"duration":484.154976,"end_time":"2023-08-17T08:11:28.710807","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-08-17T08:03:24.555831","version":"2.4.0"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"049f41e379df4412949f3e7743646f1a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0924014cb3aa489e837415c97d768e43":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a1847d27d9b45f8965ceacf152c8052":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7099f94837ec421c8cbacb45b201fc67","placeholder":"​","style":"IPY_MODEL_2117e2c9b9e947bc856107dbb55afd65","value":" 2/2 [00:00&lt;00:00, 132.49it/s]"}},"0dd1b1436c004b8b8b846bc121546c02":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12f85664dba44ddabceac88377bcfb4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20a9d4ae7f894b79b72034f5c332fcb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39e6de98627f4da8aeaded4c8a308504","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc1afac5869545e3b6800ae7da1c3876","value":2}},"2117e2c9b9e947bc856107dbb55afd65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2412e77542364709b2bf78ef71f3e8d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8719b1d6ad154829a9c3036f7a51caf9","placeholder":"​","style":"IPY_MODEL_d955e524455142b8a8ac780ff7787844","value":"100%"}},"38ac3b920db84cb9a544d7b8b3742f58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad69bacf308246deb62ffad6fc87a8d8","placeholder":"​","style":"IPY_MODEL_708f4a95ff48429ca992a7f3f68a17d8","value":"Extracting data files: 100%"}},"39e6de98627f4da8aeaded4c8a308504":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b323ac97fa143ffa1b06f5d0f296d83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d82e6f2ec56485c8a69aaf6c9b2c049","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_958b3b07653748fda3f4e5c38e19c695","value":2}},"4e22ba5de20d454a8a4f6da24fa5295a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66b936fa0d564f4abf57c4970faf7a17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_049f41e379df4412949f3e7743646f1a","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e22ba5de20d454a8a4f6da24fa5295a","value":2}},"6e86b1ad06b64765a90b7418f5451b05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"708f4a95ff48429ca992a7f3f68a17d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7099f94837ec421c8cbacb45b201fc67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"822f845e65634d72b4886a5cba03bbe3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b52ded30e83a498f8b6ec9ef7d57e0c4","placeholder":"​","style":"IPY_MODEL_6e86b1ad06b64765a90b7418f5451b05","value":"Downloading data files: 100%"}},"8719b1d6ad154829a9c3036f7a51caf9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"890df24c7f6a4896aeb9cb2fb23930c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89c2ec2daea547ea933d56cede1be921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38ac3b920db84cb9a544d7b8b3742f58","IPY_MODEL_66b936fa0d564f4abf57c4970faf7a17","IPY_MODEL_0a1847d27d9b45f8965ceacf152c8052"],"layout":"IPY_MODEL_b1011801d26948a595ccaf238b73909a"}},"8d82e6f2ec56485c8a69aaf6c9b2c049":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ec9eb9ca1934e0f9bc99aaa0bfc0e3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_822f845e65634d72b4886a5cba03bbe3","IPY_MODEL_20a9d4ae7f894b79b72034f5c332fcb2","IPY_MODEL_95421c91b0044ed19133d6bb081e5e63"],"layout":"IPY_MODEL_0dd1b1436c004b8b8b846bc121546c02"}},"95421c91b0044ed19133d6bb081e5e63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_890df24c7f6a4896aeb9cb2fb23930c2","placeholder":"​","style":"IPY_MODEL_9c71ff1192874844a6a2244a224f8b98","value":" 2/2 [00:00&lt;00:00, 136.79it/s]"}},"958b3b07653748fda3f4e5c38e19c695":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c71ff1192874844a6a2244a224f8b98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab8837168c634869ae6d33140973b0d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfd30898813244c88410920b696ae558","placeholder":"​","style":"IPY_MODEL_12f85664dba44ddabceac88377bcfb4a","value":" 2/2 [00:00&lt;00:00, 76.64it/s]"}},"ad69bacf308246deb62ffad6fc87a8d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1011801d26948a595ccaf238b73909a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b212f7f4111145d3bb3d68db629e0dce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2412e77542364709b2bf78ef71f3e8d9","IPY_MODEL_4b323ac97fa143ffa1b06f5d0f296d83","IPY_MODEL_ab8837168c634869ae6d33140973b0d8"],"layout":"IPY_MODEL_0924014cb3aa489e837415c97d768e43"}},"b52ded30e83a498f8b6ec9ef7d57e0c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc1afac5869545e3b6800ae7da1c3876":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d955e524455142b8a8ac780ff7787844":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfd30898813244c88410920b696ae558":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":5}
